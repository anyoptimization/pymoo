{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    ".. _nb_algorithms_hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/restructuredtext",
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    ".. admonition:: Info\n",
    "    :class: myOwnStyle\n",
    "    \n",
    "    Hyperparameter optimization is a new feature available since version **0.6.0**. In general, this is quite a challenging and computationally expensive topic, and only a few basics are presented in this guide. If you are interested in contributing or collaborating, please let us know to enrich this module with more robust and better features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Most algoriths have **hyperparameters**. For some optimization methods the parameters are already defined and can directly be optimized. For instance, for Differential Evolution (DE) the parameters can be found by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mating.jitter': <pymoo.core.variable.Choice at 0x7fddb9d20f10>,\n",
       " 'mating.CR': <pymoo.core.variable.Real at 0x7fddb9d20eb0>,\n",
       " 'mating.crossover': <pymoo.core.variable.Choice at 0x7fddb9b32ac0>,\n",
       " 'mating.F': <pymoo.core.variable.Real at 0x7fddb9d20e20>,\n",
       " 'mating.n_diffs': <pymoo.core.variable.Choice at 0x7fddb9d20dc0>,\n",
       " 'mating.selection': <pymoo.core.variable.Choice at 0x7fddb9d20d60>}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pymoo.algorithms.soo.nonconvex.de import DE\n",
    "from pymoo.core.parameters import get_params, flatten, set_params, hierarchical\n",
    "\n",
    "algorithm = DE()\n",
    "flatten(get_params(algorithm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If not provided directly, when initializing a `HyperparameterProblem` these variables are directly used for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Secondly, one needs to define what exactly should be optimized. For instance, for a single run on a problem (with a fixed random seed) using the well-known parameter optimization toolkit [Optuna](https://optuna.org), the implementation may look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mutation.eta': 24.01087920319256, 'mutation.prob': 0.31947746041945063, 'crossover.zeta': 0.19712354833387533, 'crossover.eta': 0.06510354041781266, 'family_size': 9, 'n_parents': 10, 'n_offsprings': 2, 'pop_size': 122}\n",
      "Best solution found: \n",
      "X = [0.49931263 0.49971117 0.49995666 0.50012005 0.49995485 0.49954006\n",
      " 0.50013753 0.49992644 0.50004025 0.50003723]\n",
      "F = [8.13108862e-07]\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.hyperparameters import SingleObjectiveSingleRun, HyperparameterProblem\n",
    "from pymoo.algorithms.soo.nonconvex.g3pcx import G3PCX\n",
    "from pymoo.algorithms.soo.nonconvex.optuna import Optuna\n",
    "from pymoo.core.parameters import set_params, hierarchical\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.problems.single import Sphere\n",
    "\n",
    "algorithm = G3PCX()\n",
    "\n",
    "problem = Sphere(n_var=10)\n",
    "n_evals = 500\n",
    "\n",
    "performance = SingleObjectiveSingleRun(problem, termination=(\"n_evals\", n_evals), seed=1)\n",
    "\n",
    "res = minimize(HyperparameterProblem(algorithm, performance),\n",
    "               Optuna(),\n",
    "               termination=('n_evals', 50),\n",
    "               seed=1,\n",
    "               verbose=False)\n",
    "\n",
    "hyperparams = res.X\n",
    "print(hyperparams)\n",
    "set_params(algorithm, hierarchical(hyperparams))\n",
    "\n",
    "res = minimize(Sphere(), algorithm, termination=(\"n_evals\", n_evals), seed=1)\n",
    "print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Of course, you can also directly use the `MixedVariableGA` available in our framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mutation.eta': 10.202478525408438, 'mutation.prob': 0.18217181670454258, 'crossover.zeta': 0.2672212328003948, 'crossover.eta': 0.038505365248462986, 'family_size': 10, 'n_parents': 4, 'n_offsprings': 5, 'pop_size': 169}\n",
      "Best solution found: \n",
      "X = [0.49931151 0.49958918 0.49947128 0.50076286 0.50029319 0.50035433\n",
      " 0.4995032  0.49959396 0.49910168 0.50016305]\n",
      "F = [2.96104366e-06]\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.hyperparameters import SingleObjectiveSingleRun, HyperparameterProblem\n",
    "from pymoo.algorithms.soo.nonconvex.g3pcx import G3PCX\n",
    "from pymoo.algorithms.soo.nonconvex.optuna import Optuna\n",
    "from pymoo.core.mixed import MixedVariableGA\n",
    "from pymoo.core.parameters import set_params, hierarchical\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.problems.single import Sphere\n",
    "\n",
    "\n",
    "algorithm = G3PCX()\n",
    "\n",
    "problem = Sphere(n_var=10)\n",
    "n_evals = 500\n",
    "\n",
    "performance = SingleObjectiveSingleRun(problem, termination=(\"n_evals\", n_evals), seed=1)\n",
    "\n",
    "res = minimize(HyperparameterProblem(algorithm, performance),\n",
    "               MixedVariableGA(pop_size=5),\n",
    "               termination=('n_evals', 50),\n",
    "               seed=1,\n",
    "               verbose=False)\n",
    "\n",
    "hyperparams = res.X\n",
    "print(hyperparams)\n",
    "set_params(algorithm, hierarchical(hyperparams))\n",
    "\n",
    "res = minimize(Sphere(), algorithm, termination=(\"n_evals\", n_evals), seed=1)\n",
    "print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, optimizing the parameters for a **single random seed** is often not desirable. And this is precisely what makes hyper-parameter optimization computationally expensive. So instead of using just a single random seed, we can use the `MultiRun` performance assessment to average over multiple runs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "n_gen  |  n_eval  |     f_avg     |     f_min    \n",
      "=================================================\n",
      "     1 |        5 |  0.0011469191 |  0.0001158058\n",
      "     2 |       10 |  0.0003004380 |  1.324378E-06\n",
      "     3 |       15 |  0.0001083746 |  1.324378E-06\n",
      "     4 |       20 |  0.0000604004 |  1.324378E-06\n",
      "     5 |       25 |  6.940028E-06 |  1.324378E-06\n",
      "     6 |       30 |  2.363478E-06 |  1.324378E-06\n",
      "     7 |       35 |  2.184765E-06 |  1.324378E-06\n",
      "     8 |       40 |  1.801311E-06 |  1.290965E-06\n",
      "     9 |       45 |  1.457222E-06 |  7.690855E-07\n",
      "    10 |       50 |  1.457222E-06 |  7.690855E-07\n",
      "{'mutation.eta': 3.003840525176007, 'mutation.prob': 0.2613802441259011, 'crossover.zeta': 0.2092756091839343, 'crossover.eta': 0.13100894947710487, 'family_size': 9, 'n_parents': 7, 'n_offsprings': 4, 'pop_size': 63}\n",
      "Best solution found: \n",
      "X = [0.50058063 0.50049996 0.49944047 0.49958107 0.50008284 0.50007215\n",
      " 0.49972702 0.50003434 0.49996608 0.50038491]\n",
      "F = [1.31273882e-06]\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.hyperparameters import HyperparameterProblem, MultiRun, stats_single_objective_mean\n",
    "from pymoo.algorithms.soo.nonconvex.g3pcx import G3PCX\n",
    "from pymoo.core.mixed import MixedVariableGA\n",
    "from pymoo.core.parameters import set_params, hierarchical\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.problems.single import Sphere\n",
    "\n",
    "\n",
    "algorithm = G3PCX()\n",
    "\n",
    "problem = Sphere(n_var=10)\n",
    "n_evals = 500\n",
    "seeds = [5, 50, 500]\n",
    "\n",
    "performance = MultiRun(problem, seeds=seeds, func_stats=stats_single_objective_mean, termination=(\"n_evals\", n_evals))\n",
    "\n",
    "res = minimize(HyperparameterProblem(algorithm, performance),\n",
    "               MixedVariableGA(pop_size=5),\n",
    "               termination=('n_evals', 50),\n",
    "               seed=1,\n",
    "               verbose=True)\n",
    "\n",
    "hyperparams = res.X\n",
    "print(hyperparams)\n",
    "set_params(algorithm, hierarchical(hyperparams))\n",
    "\n",
    "res = minimize(Sphere(), algorithm, termination=(\"n_evals\", n_evals), seed=5)\n",
    "print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another way of performance measure is the number of evaluations until a specific goal has been reached. For single-objective optimization, such a goal is most likely until a minimum function value has been found. Thus, for the termination, we use `MinimumFunctionValueTermination` with a value of `1e-5`. We run the method for each random seed until this value has been reached or at most `500` function evaluations have taken place. The performance is then measured by the average number of function evaluations (`func_stats=stats_avg_nevals`) to reach the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================\n",
      "n_gen  |  n_eval  |     f_avg     |     f_min    \n",
      "=================================================\n",
      "     1 |        5 |  5.298000E+02 |  5.030000E+02\n",
      "     2 |       10 |  4.895333E+02 |  4.276667E+02\n",
      "     3 |       15 |  4.763333E+02 |  4.276667E+02\n",
      "     4 |       20 |  4.571333E+02 |  4.276667E+02\n",
      "     5 |       25 |  4.411333E+02 |  4.276667E+02\n",
      "     6 |       30 |  4.382000E+02 |  4.276667E+02\n",
      "     7 |       35 |  4.360667E+02 |  4.276667E+02\n",
      "     8 |       40 |  4.200667E+02 |  3.850000E+02\n",
      "     9 |       45 |  4.068667E+02 |  3.850000E+02\n",
      "    10 |       50 |  4.068667E+02 |  3.850000E+02\n",
      "{'mutation.eta': 22.546590161392896, 'mutation.prob': 0.13789985939149485, 'crossover.zeta': 0.20871365511506024, 'crossover.eta': 0.12039717249146664, 'family_size': 10, 'n_parents': 7, 'n_offsprings': 4, 'pop_size': 65}\n",
      "Best solution found: \n",
      "X = [0.5005912  0.499759   0.50062838 0.49857344 0.49949559 0.50094711\n",
      " 0.49905868 0.49954207 0.49892447 0.50003376]\n",
      "F = [6.24263744e-06]\n"
     ]
    }
   ],
   "source": [
    "from pymoo.algorithms.hyperparameters import HyperparameterProblem, MultiRun, stats_avg_nevals\n",
    "from pymoo.algorithms.soo.nonconvex.g3pcx import G3PCX\n",
    "from pymoo.core.mixed import MixedVariableGA\n",
    "from pymoo.core.parameters import set_params, hierarchical\n",
    "from pymoo.core.termination import TerminateIfAny\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.problems.single import Sphere\n",
    "from pymoo.termination.fmin import MinimumFunctionValueTermination\n",
    "from pymoo.termination.max_eval import MaximumFunctionCallTermination\n",
    "\n",
    "algorithm = G3PCX()\n",
    "\n",
    "problem = Sphere(n_var=10)\n",
    "\n",
    "termination = TerminateIfAny(MinimumFunctionValueTermination(1e-5), MaximumFunctionCallTermination(500))\n",
    "\n",
    "performance = MultiRun(problem, seeds=[5, 50, 500], func_stats=stats_avg_nevals, termination=termination)\n",
    "\n",
    "res = minimize(HyperparameterProblem(algorithm, performance),\n",
    "               MixedVariableGA(pop_size=5),\n",
    "               ('n_evals', 50),\n",
    "               seed=1,\n",
    "               verbose=True)\n",
    "\n",
    "hyperparams = res.X\n",
    "print(hyperparams)\n",
    "set_params(algorithm, hierarchical(hyperparams))\n",
    "\n",
    "res = minimize(Sphere(), algorithm, termination=(\"n_evals\", res.f), seed=5)\n",
    "print(\"Best solution found: \\nX = %s\\nF = %s\" % (res.X, res.F))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}